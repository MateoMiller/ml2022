{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal, assert_allclose\n",
    "from pandas.testing import assert_frame_equal\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать свой K-means.\n",
    "\n",
    "В `init` устанавливается кол-во кластеров\n",
    "\n",
    "В `fit` происходит цикл обучения:\n",
    "\n",
    "1. Выбор $k$ центральных точек (первый раз произвольно)\n",
    "\n",
    "2. Ищем расстояния каждой точки выборки до всех центров\n",
    "\n",
    "3. Разбиваем все точки на множества по признаку расстояния до ближайшего центра (тот центр что ближе по евклидовой метрике, тот кластер и выбираем)\n",
    "\n",
    "4. Обозначаем новые центры, как среднюю точку получившихся кластеров\n",
    "\n",
    "5. Если новые центры не сильно отличаются (сумма расстояний между соответствующими центрами не превышает `1e-3`), то заканиваем цикл, иначе повторяем цикл с шага 2 с новыми центрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, clusters=2, eps=1e-3):\n",
    "        self.clusters_ = clusters\n",
    "        self.labels_ = None\n",
    "        self.X_ = None\n",
    "        self.eps_ = eps\n",
    "\n",
    "    def _update_centers(self, X, centers):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return new_centers, labels\n",
    "\n",
    "    def fit(self, X: np.array):\n",
    "        self.X_ = X\n",
    "        centers_idx = np.random.choice(np.arange(len(self.X_)), self.clusters_)\n",
    "\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        while norm(new_centers - centers, axis=1).sum() > self.eps_:\n",
    "            ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([ 3, 3])\n",
    "X2 = np.random.randn(N,2) + np.array([-3,-3])\n",
    "X3 = np.random.randn(N,2) + np.array([-3, 3])\n",
    "X4 = np.random.randn(N,2) + np.array([ 3,-3])\n",
    "\n",
    "X = np.vstack([X1, X2, X3, X4])\n",
    "\n",
    "model = KMeans(4)\n",
    "\n",
    "model.fit(X)\n",
    "labels = model.labels_\n",
    "\n",
    "answer = labels.reshape((4,300)).mean(axis=1)\n",
    "\n",
    "answer.sort()\n",
    "assert_almost_equal(answer[0], 0, decimal=1)\n",
    "assert_almost_equal(answer[1], 1, decimal=1)\n",
    "assert_almost_equal(answer[2], 2, decimal=1)\n",
    "assert_almost_equal(answer[3], 3, decimal=1)\n",
    "################################################\n",
    "\n",
    "N = 100\n",
    "\n",
    "X1 = np.random.randn(N,1) + np.array([ 2, 0])\n",
    "X2 = np.random.randn(N,1) + np.array([-2, 0])\n",
    "\n",
    "\n",
    "X = np.vstack([X1, X2])\n",
    "\n",
    "model = KMeans(2)\n",
    "\n",
    "model.fit(X)\n",
    "labels = model.labels_\n",
    "\n",
    "assert labels[:101].sum() / 100 < 0.1 or labels[101:].sum() / 100 < 0.1\n",
    "################################################\n",
    "\n",
    "X1 = [[1,2], [1,1], [1,0], [1, -1], [1,-2]]\n",
    "X2 = [[-1,2], [-1,1], [-1,0], [-1, -1], [-1,-2]]\n",
    "X = np.vstack([X1, X2])\n",
    "\n",
    "model = KMeans(2)\n",
    "\n",
    "centers = [[1,1], [-1,-1]]\n",
    "\n",
    "new_centers, labels = model._update_centers(X, centers)\n",
    "\n",
    "assert_almost_equal(new_centers, np.array([[ 0.33,  0.83], [-0.5, -1.25]]), decimal=2)\n",
    "\n",
    "assert_array_equal(labels, [0,0,0,0,1,0,0,1,1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании необходимо реализовать простой вариант DBSCAN.\n",
    "\n",
    "0. Обозначили **кластер шума** инексом $0$, а текущий кластер $C$ = $1$. Выбрали произвольную точку $p$.\n",
    "\n",
    "1. Найдем всех **соседей** точки $p$, которые находятся на расстоянии не более чем `eps`.\n",
    "\n",
    "2. Если соседей **меньше** чем `min_samples`, то мы обозначаем точку $p$ **шумом** (кластер $0$) и возвращаемся к шагу 1.\n",
    "\n",
    "3. Если соседей `min_samples` или **более**, то обозначаем точку $p$ номером текущего кластера $C$, а соседей добавляем в **очередь**.\n",
    "\n",
    "4. Пока очередь не пустая, берем новую точку $p$ и возвращаемся к шагу 2.\n",
    "\n",
    "5. Если очередь пустая, то увеличиваем индекс текущего класса $C$ и возвращаемся к шаг 1.\n",
    "\n",
    "6. Алгоритм заканчивает работу, когда неразмеченных точек не осталось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN():\n",
    "    def __init__(self, eps=0.5, min_samples=5):\n",
    "        self.eps_ = eps\n",
    "        self.min_samples_ = min_samples\n",
    "        self.distance_ = lambda x, y: np.sqrt((x[0] - y[0])**2 +  (x[1] - y[1])**2)\n",
    "        self.clusters = {0:[]} #здесь должны лежать лист индексов точек, которые попадают в один кластер\n",
    "        self.labels_ = None #лейблы по индексам точек\n",
    "\n",
    "    def _get_neighbours_ind(self, point_ind, X):\n",
    "        return [ind for ind in range(X.shape[0])\n",
    "                if self.distance_(X[point_ind], X[ind]) < self.eps_]\n",
    "\n",
    "    def fit(self, X: np.array):\n",
    "        self.labels_     = np.zeros(X.shape[0])\n",
    "        self.visited_ind = np.zeros(X.shape[0])\n",
    "\n",
    "        C = 1\n",
    "\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        for root in range(X.shape[0]):\n",
    "\n",
    "            ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "            while neighbours:\n",
    "                new_ind = neighbours.pop()\n",
    "                if self.visited_ind[new_ind]:\n",
    "                    continue\n",
    "\n",
    "                ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "theta = np.linspace(0, 2*np.pi, num_samples)\n",
    "\n",
    "r1 = 1\n",
    "r2 = 2\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "circle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 32),\n",
    "                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 32)])\n",
    "lil = r1 * circle\n",
    "big = r2 * circle\n",
    "\n",
    "X_circle = np.vstack([lil, big])\n",
    "\n",
    "model = DBSCAN(eps=0.3, min_samples=4).fit(X_circle)\n",
    "\n",
    "labels_circle = model.labels_\n",
    "\n",
    "answer = labels_circle.reshape((2,200)).mean(axis=1)\n",
    "\n",
    "answer.sort()\n",
    "assert_almost_equal(answer[0], 1, decimal=2)\n",
    "assert_almost_equal(answer[1], 2, decimal=2)\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "X = [[np.random.randn()/6, np.random.randn()/6] for i in range(150)]\n",
    "X.extend([[np.random.randn()/4 + 2.5, np.random.randn()/5] for i in range(150)])\n",
    "X.extend([[np.random.randn()/5 + 1, np.random.randn()/5 + 1] for i in range(150)])\n",
    "X.extend([[i/25 - 1, + np.random.randn()/20 - 1] for i in range(150)])\n",
    "X.extend([[i/25 - 2.5, 3 - ((i+30)/60 - 2)**2 + np.random.randn()/20] for i in range(150)])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "model = DBSCAN(eps=0.2, min_samples=4).fit(X)\n",
    "\n",
    "answer = model.labels_.reshape((5,150)).mean(axis=1)\n",
    "\n",
    "answer.sort()\n",
    "assert_almost_equal(answer[0], 1, decimal=1)\n",
    "assert_almost_equal(answer[1], 2, decimal=1)\n",
    "assert_almost_equal(answer[2], 3, decimal=1)\n",
    "assert_almost_equal(answer[3], 4, decimal=1)\n",
    "assert_almost_equal(answer[4], 5, decimal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании необходимо реализовать EM-алгоритм для двумерного случая (на самом деле можно и большемерного, но все тесты будут двумерные).\n",
    "\n",
    "На E-шаге необходимо пересчитать вероятности попадания каждого объекта в каждый из классов (при фиксированных $\\mu$ и $\\Sigma$ для каждого класса). На выходе мы получаем лист длины `n_clusters`, каждый объект которого - np.array c текущими вероятностями.\n",
    "\n",
    "$$g[c,i] = P(y=c|x_i) = \\frac{w_c p(x_i|\\mu_c, \\Sigma_c)}{\\sum_{s \\in C} w_s p(x_i|\\mu_s, \\Sigma_s)}$$\n",
    "\n",
    "где\n",
    "* С - множество классов\n",
    "* p(x|\\mu_c, \\Sigma_c) - многомерное нормальное распределение с матожиданием $\\mu_c$ и матрицей ковариаций $\\Sigma_c$\n",
    "\n",
    "На М-шаге нам нужно пересчитать наши параметры распределений:\n",
    "\n",
    "* Априорная вероятность конкретного класса (массив вероятностей)\n",
    "    $$w_c = P(y=c) = \\frac{1}{N}\\sum^{N}_{i}g[c,i]$$\n",
    "\n",
    "* Новые матожидания (лист массивов, который содержит матожидание для 1 и 2 размерности)\n",
    "    $$\\mu_c =  \\frac{1}{N w_c}\\sum^{N}_{i}g[c,i]x_i$$\n",
    "\n",
    "* Новые матрицы ковариаций (лист массивов, который содержит матожидание для 1 и 2 размерности)\n",
    "    $$\\Sigma_c =  \\frac{1}{N w_c}\\sum^{N}_{i}g[c,i]f(x_i - \\mu_c)$$\n",
    "где $f(x) = xx^T$ - матрица попарных перемножений\n",
    "$$f([a1,a2]) = \\begin{bmatrix}\n",
    "    a1a1  & a1a2 \\\\\n",
    "    a2a1  & a2a2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Задача очевидно может не зайти с первого раза. Отработайте локально, что задача в большинстве случаев заходит и несколько раз переотправьте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "class EM_algo():\n",
    "    def __init__(self, n_clusters=2, n_iter=10):\n",
    "        self.n_clusters_ = n_clusters\n",
    "        self.n_iter = n_iter\n",
    "        self.mu_ = None # лист векторов матожиданий List[np.array]\n",
    "        self.cov_ = None # лист матриц ковариаций List[np.array]\n",
    "        self.w_ = np.ones(n_clusters) * 1 / n_clusters #изначально заполняем w-шки равномерно\n",
    "        self.X_ = None # запоминаем выборку\n",
    "\n",
    "    def _estep(self) -> List[np.array]:\n",
    "        for i in range(self.n_clusters_):\n",
    "            p = multivariate_normal.pdf( ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ# вероятности кластера i для каждого объекта, вектор размера выборки\n",
    "            ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        return g\n",
    "\n",
    "    def _mstep(self, g):\n",
    "        self.w_ = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        self.mu_ = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        for i in range(self.n_clusters_):\n",
    "            dX = self.X_ - self.mu_[i]\n",
    "            self.cov_[i] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        return self.mu_, self.cov_\n",
    "\n",
    "    def fit(self, X: np.array):\n",
    "        self.X_ = X\n",
    "        self.mu_  = [random.choice(X) for _ in range(self.n_clusters_)] #берем произвольные точки как центры наших кластеров List[np.array]\n",
    "        self.cov_ = [np.cov(np.transpose(X)) for _ in range(self.n_clusters_)] #считаем матрицу ковариаций для каждого распределения List[np.array]\n",
    "        for _ in range(self.n_iter):\n",
    "            self._mstep(self._estep())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x01 = [[1,1],[1.1,1.1],[0.9,0.9],[1,1.2]]\n",
    "x02 = [[7,7],[7.1,7.1],[6.9,6.9],[7,7.2]]\n",
    "X0 = np.concatenate((x01, x02), axis=0)\n",
    "\n",
    "model = EM_algo(n_clusters=2, n_iter=2)\n",
    "model.mu_  = [[2,2],[5,5]]\n",
    "\n",
    "model.cov_ = [[[3, 1], [-1, 3]],\n",
    "              [[3, 3], [0.5, 10]]]\n",
    "model.X_ = X0\n",
    "\n",
    "g = model._estep()\n",
    "\n",
    "assert_almost_equal(g, [[0.967, 0.965, 0.969, 0.968, 0., 0., 0., 0.],\n",
    "                        [0.033, 0.035, 0.031, 0.032, 1.,1., 1., 1.]], decimal=2)\n",
    "\n",
    "w, mu, cov = model._mstep(g)\n",
    "\n",
    "assert_almost_equal(w, [0.484, 0.516], decimal=2)\n",
    "assert_almost_equal(mu, [[1., 1.05], [6.81, 6.86]], decimal=2)\n",
    "\n",
    "assert_almost_equal(cov, [[[0.006, 0.006], [0.006, 0.013]], [[1.107, 1.107], [1.107, 1.115]]], decimal=2)\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "m1 = [1 ,1]      # consider a random mean and covariance value\n",
    "m2 = [7 ,7]\n",
    "cov1 = [[1, 0.5], [0.5, 1]]\n",
    "cov2 = [[1,-0.5], [-0.5, 1]]\n",
    "x11 = np.random.multivariate_normal(m1, cov1, size=(200,))  # Generating 200 samples for each mean and covariance\n",
    "x12 = np.random.multivariate_normal(m2, cov2, size=(200,))\n",
    "X1 = np.concatenate((x11, x12), axis=0)\n",
    "\n",
    "model = EM_algo(n_clusters=2, n_iter=30)\n",
    "\n",
    "model.fit(X1)\n",
    "\n",
    "\n",
    "if model.mu_[0][0] < model.mu_[1][0]:\n",
    "    assert_allclose(model.mu_, [m1, m2], atol=0.5)\n",
    "    assert_allclose(model.cov_, [cov1, cov2], atol=0.5)\n",
    "else:\n",
    "    assert_allclose(model.mu_, [m2, m1], atol=0.5)\n",
    "    assert_allclose(model.cov_, [cov2, cov1], atol=0.5)\n",
    "\n",
    "##################################################################3\n",
    "\n",
    "m1 = [0 ,8]\n",
    "m2 = [1 ,1]      # consider a random mean and covariance value\n",
    "m3 = [7 ,7]\n",
    "\n",
    "cov1 = np.array([[1, 2], [2, 1]]) * 1/4\n",
    "cov2 = np.array([[3, 2], [2, 3]]) * 1/4\n",
    "cov3 = np.array([[2, -1], [-1, 2]]) * 1/4\n",
    "\n",
    "x21 = np.random.multivariate_normal(m1, cov1, size=(50,))  # Generating 200 samples for each mean and covariance\n",
    "x22 = np.random.multivariate_normal(m2, cov2, size=(50,))\n",
    "x23 = np.random.multivariate_normal(m3, cov3, size=(50,))\n",
    "\n",
    "X2 = np.concatenate((x21, x22, x23), axis=0)\n",
    "\n",
    "model = EM_algo(n_clusters=3, n_iter=30)\n",
    "\n",
    "model.fit(X2)\n",
    "\n",
    "sort_idx = model.mu_[:, 0].argsort()\n",
    "assert_allclose(model.mu_[sort_idx], [m1, m2, m3], atol=0.5)\n",
    "assert_allclose(np.array(model.cov_)[sort_idx], [cov1, cov2, cov3], atol=0.5)\n",
    "\n",
    "##################################################################3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standart Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании необходимо сделать стандартные фичи усреднения для врменного ряда. Не забудьте сделать shift, чтобы фичи были ДО предсказзываемого значения.\n",
    "\n",
    "* `SMA_n` - экспоненциальное среднее на последних n днях\n",
    "\n",
    "* `EMA_n` - экспоненциальное среднее при параметре `com=9` (тоже самое что `alpha=1/(1 + com)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_averages(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['EMA_9']  = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['SMA_5']  = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['SMA_10'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['SMA_15'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['SMA_30'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nflx.us.txt', sep=',')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df[(df['Date'].dt.year >= 2015)].copy()\n",
    "df.index = range(len(df))\n",
    "\n",
    "answer = ts_averages(df)[['Close','EMA_9', 'SMA_5', 'SMA_10', 'SMA_15', 'SMA_30']][400:].reset_index(drop=True)\n",
    "\n",
    "assert_frame_equal(answer, pd.read_csv('data/answer_ma.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследовательская задача. Вам необходимо разобраться в 2-х известных фичах, для предсказания валют и реализовать. Даны предварительные ссылки на википедию, но можно юзать все что угодно.\n",
    "\n",
    "* `RSI` - [Индекс_относительной_силы](https://ru.wikipedia.org/wiki/Индекс_относительной_силы)\n",
    "\n",
    "* `MACD = EMA_12 - EMA_26` - смотри описание по ссылка ниже\n",
    "\n",
    "* `MACD_signal` - [Индикатор_MACD](https://ru.wikipedia.org/wiki/Индикатор_MACD)\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "* в `RSI` нужно делать `rolling n=14` (14 дневный стандартный период)\n",
    "\n",
    "* В `RSI` У понижающих цен не забудьте взять abs\n",
    "\n",
    "* Для подсчета `EMA_m` нужно установить `span=m` и `min_periods=m`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['RSI'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['MACD'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    df['MACD_signal'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nflx.us.txt', sep=',')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df[(df['Date'].dt.year >= 2015)].copy()\n",
    "df.index = range(len(df))\n",
    "\n",
    "answer = extra_features(df)[['Close','RSI', 'MACD_signal']][400:].reset_index(drop=True)\n",
    "\n",
    "assert_frame_equal(answer, pd.read_csv('data/answer_rsi_macd.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSprediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "\n",
    "* Сгенерировать таргет Close+1 (не забудьте убрать последний элемент)\n",
    "\n",
    "* Сгенерируйте фичи по функциям из предыдущих задач\n",
    "\n",
    "* Оставить данные только начиная с 2016 года, чтобы не было проблем с Nan\n",
    "\n",
    "* Убрать все фичи, кроме тех, что мы сгенерировали в предыдущих заданиях\n",
    "\n",
    "* Разбить на трейн и тест в отношении 0.85:0.15\n",
    "\n",
    "* Обучить CatboostRegressor и вернуть его\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def ts_catboost(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['Close+1'] = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "    X_train, X_test, y_train, y_test = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "    model = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "df = pd.read_csv('data/nflx.us.txt', sep=',')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df[(df['Date'].dt.year >= 2015)].copy()\n",
    "df.index = range(len(df))\n",
    "\n",
    "model, X_test, y_test = ts_catboost(df)\n",
    "\n",
    "assert X_test.shape == (71, 8)\n",
    "\n",
    "assert_array_equal(y_test[-10:], [198.37, 196.43, 198.00, 199.32, 200.01, 200.13, 195.89, 196.44, 193.90, 192.02])\n",
    "\n",
    "assert MSE(model.predict(X_test), y_test) < 900"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
